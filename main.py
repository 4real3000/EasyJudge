import streamlit as st
import ollama
import re
import json
import plotly.express as px
from rouge_score import rouge_scorer
import sacrebleu
from bert_score import score as bert_score
import pandas as pd
import plotly.graph_objects as go

###ç»˜å›¾###
def compute_rouge(reference, candidate):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, candidate)
    return {key: score.fmeasure for key, score in scores.items()}

def compute_bertscore(reference, candidate):
    P, R, F1 = bert_score([candidate], [reference], lang='en', model_type='bert-base-uncased', verbose=True)
    return F1.mean().item()

def generate_metrics_plot(rouge_scores, bleu_score, bert_score):
    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', 'BERTScore']
    scores = [
        rouge_scores['rouge1'],
        rouge_scores['rouge2'],
        rouge_scores['rougeL'],
        bleu_score,
        bert_score,
    ]
    
    df = pd.DataFrame({
        'Metrics': metrics,
        'Scores': scores
    })
    
    fig = px.bar(df, x='Metrics', y='Scores', text='Scores',
                 #title="Text Generation Evaluation Metrics",
                 color='Scores', color_continuous_scale=px.colors.sequential.Viridis)
    fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
    fig.update_layout(
        uniformtext_minsize=8, 
        uniformtext_mode='hide',
        xaxis_title='',  # å»æ‰xè½´æ ‡é¢˜
        margin=dict(l=0, r=0, t=0, b=0),  # Reduce margins to use more space for the chart
        height=350,
    )
    return fig


def plot_scores_PAIRWISE(processed_response, for_chart):
    multi_dimension_score = {
                        'score_A': processed_response['score_A'],
                        'score_B': processed_response['score_B']
                    }
    metrics = list(multi_dimension_score['score_A'].keys())
    scores_A = [multi_dimension_score['score_A'][metric] for metric in metrics]
    scores_B = [multi_dimension_score['score_B'][metric] for metric in metrics]
    
    # åˆ›å»º Plotly å›¾è¡¨
    fig1 = go.Figure(data=[
        go.Bar(name='Score A', x=metrics, y=scores_A),
        go.Bar(name='Score B', x=metrics, y=scores_B)
    ])
    
    # æ›´æ–°å›¾è¡¨å¸ƒå±€
    fig1.update_layout(
        barmode='group',
        #title='Score Comparison for A vs B',
        #xaxis_title='Metric',
        yaxis_title='Scores',
        legend_title='Score Groups',
        margin=dict(l=10, r=10, t=30, b=10),
        height=400
    )
    
    # ä½¿ç”¨ Streamlit çš„ st.plotly_chart() æ¥æ˜¾ç¤ºå›¾è¡¨
    #st.plotly_chart(fig)

    #ç¬¬ä¸€å¼ å’Œreferenceæ¯”è¾ƒå›¾
    reference = for_chart["reference"]
    candidate1 = for_chart["answer1"]
    rouge_scores = compute_rouge(reference, candidate1)
    bleu_score = sacrebleu.corpus_bleu([candidate1], [[reference]]).score
    bert_score_val = compute_bertscore(reference, candidate1)
    fig2 = generate_metrics_plot(rouge_scores, bleu_score/10, bert_score_val)

    #ç¬¬äºŒå¼ å’Œreferenceæ¯”è¾ƒå›¾
    reference = for_chart["reference"]
    candidate1 = for_chart["answer2"]
    rouge_scores = compute_rouge(reference, candidate1)
    bleu_score = sacrebleu.corpus_bleu([candidate1], [[reference]]).score
    bert_score_val = compute_bertscore(reference, candidate1)
    fig3 = generate_metrics_plot(rouge_scores, bleu_score/10, bert_score_val)

    return fig1, fig2, fig3
    

def plot_scores_POINTWISE(processed_response, for_chart):
    # å‡è®¾ processed_response åŒ…å« 'score_A' æˆ–ç±»ä¼¼å­—æ®µ
    dimension_scores = processed_response["Dimension_Scores"]  # ä½¿ç”¨ processed_response ä½œä¸ºè¾“å…¥æ•°æ®
    
    metrics = list(dimension_scores.keys())
    scores = list(dimension_scores.values())

    # åˆ›å»º Plotly å›¾è¡¨
    fig1 = go.Figure(data=[
        go.Bar(x=metrics, y=scores)
    ])

    # æ›´æ–°å›¾è¡¨å¸ƒå±€
    fig1.update_layout(
        #title='Dimension Scores',
        #xaxis_title='Metric',
        yaxis_title='Score',
        xaxis=dict(type='category'),  # ç¡®ä¿xè½´ä¸ºç±»åˆ«ç±»å‹
        yaxis=dict(range=[0, max(scores) + 1]),  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´yè½´èŒƒå›´
        margin=dict(l=10, r=10, t=30, b=10),
        height=400
    )
    
    # ä½¿ç”¨ Streamlit çš„ st.plotly_chart() æ–¹æ³•æ˜¾ç¤ºå›¾è¡¨
    #st.plotly_chart(fig)
     #ç¬¬ä¸€å¼ å’Œreferenceæ¯”è¾ƒå›¾
    reference = for_chart["reference"]
    candidate1 = for_chart["answer"]
    rouge_scores = compute_rouge(reference, candidate1)
    bleu_score = (sacrebleu.corpus_bleu([candidate1], [[reference]]).score)/10
    bert_score_val = compute_bertscore(reference, candidate1)
    fig2 = generate_metrics_plot(rouge_scores, bleu_score, bert_score_val)

    return fig1,fig2


# Function to extract required parts from gpt_response
def extract_gpt_response_info_pairwise(gpt_response):
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç”¨äºæå–å„éƒ¨åˆ†
    pattern_a = r"@@@(.*?)@@@"
    pattern_b = r"@@@(.*?)###"
    pattern_final_result = r"###(.*?)&&&"
    pattern_detailed_feedback = r"&&&Detailed Evaluation Feedback:(.*?)\*\*\*"
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
    match_a = re.search(pattern_a, gpt_response, re.DOTALL)
    match_b = re.search(pattern_b, gpt_response, re.DOTALL)
    match_final_result = re.search(pattern_final_result, gpt_response, re.DOTALL)
    match_detailed_feedback = re.search(pattern_detailed_feedback, gpt_response, re.DOTALL)
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    result = {}

    # å¯¹ dict_A å’Œ dict_B ä½¿ç”¨å­—ç¬¦ä¸²è§£æï¼ˆéæ ‡å‡†JSONæ ¼å¼æ— æ³•ç›´æ¥è§£æï¼‰
    dict_a_raw = match_a.group(1).strip() if match_a else ""
    dict_b_raw = match_b.group(1).strip() if match_b else ""
    
    # å°†è‡ªå®šä¹‰çš„æ ¼å¼è½¬æ¢ä¸ºé”®å€¼å¯¹å­—å…¸
    def parse_custom_format(raw_text):
        scores = {}
        # åŒ¹é…ç±»ä¼¼ 'Key': value çš„æ ¼å¼
        matches = re.findall(r"'(.*?)':\s*(\d+)", raw_text)
        for key, value in matches:
            scores[key] = int(value)
        return scores
    
    # è§£æ dict_A å’Œ dict_B
    result['score_A'] = parse_custom_format(dict_a_raw)
    result['score_B'] = parse_custom_format(dict_b_raw)
    result['final_results'] = match_final_result.group(1).strip() if match_final_result else ""
    result['Detailed_Evaluation_Feedback'] = match_detailed_feedback.group(1).strip() if match_detailed_feedback else ""
    
    return result

def extract_gpt_response_info_pointwise(gpt_response):
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç”¨äºæå–å„éƒ¨åˆ†
    pattern_dict_a = r"@@@Dimension Scores:\s*(\{.*?\})###"
    pattern_dict_b = r"###Overall Score:\s*(\d+)&&&"
    pattern_detailed_feedback = r"&&&Detailed Evaluation Feedback:(.*?)\*\*\*"

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
    match_dict_a = re.search(pattern_dict_a, gpt_response, re.DOTALL)
    match_dict_b = re.search(pattern_dict_b, gpt_response, re.DOTALL)
    match_detailed_feedback = re.search(pattern_detailed_feedback, gpt_response, re.DOTALL)

    # åˆå§‹åŒ–ç»“æœå­—å…¸
    result = {}

    # æ‰‹åŠ¨è§£æè‡ªå®šä¹‰æ ¼å¼çš„å­—å…¸
    def parse_custom_format(raw_text):
        scores = {}
        # åŒ¹é…ç±»ä¼¼ 'Key': value çš„æ ¼å¼ (å…¶ä¸­valueæ˜¯æ•´æ•°)
        matches = re.findall(r"'(.*?)':\s*(\d+)", raw_text)
        for key, value in matches:
            scores[key] = int(value)
        return scores

    # è§£æå­—å…¸A (Dimension Scores)
    dict_a_raw = match_dict_a.group(1).strip() if match_dict_a else ""
    dict_a = parse_custom_format(dict_a_raw)

    # è§£æå­—å…¸B (Overall Score)
    dict_b = {"Overall Score": int(match_dict_b.group(1).strip())} if match_dict_b else {}

    # è§£æè¯¦ç»†åé¦ˆ (Detailed Evaluation Feedback)
    detailed_feedback = match_detailed_feedback.group(1).strip() if match_detailed_feedback else ""

    # å°†è§£æçš„å†…å®¹å­˜å…¥ç»“æœå­—å…¸
    result['Dimension_Scores'] = dict_a
    result['Overall_Score'] = dict_b
    result['Detailed_Evaluation_Feedback'] = detailed_feedback

    return result


def read_criteria(scenario):
    """æ ¹æ®åœºæ™¯è¯»å–ç›¸åº”çš„è¯„ä»·æ ‡å‡†æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(f'./txt_simplify_criteria/{scenario}.txt', 'r', encoding='utf-8') as file:
            criteria = file.read()
        return criteria
    except FileNotFoundError:
        print(f"No criteria found for {scenario}")
        return ""

def user_selected_criteria(criteria_list):
    # éå†åˆ—è¡¨ï¼Œå°†æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºå¸¦æœ‰åºå·çš„æ ¼å¼
    formatted_criteria = [f"{i+1}. {criteria}" for i, criteria in enumerate(criteria_list)]
    # å°†æ‰€æœ‰å…ƒç´ åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå…ƒç´ å ä¸€è¡Œ
    return "\n".join(formatted_criteria)
# App title
st.set_page_config(page_title="ğŸ’¬ EasyJudge",layout="wide")

model_list = ollama.list()

if "model_name" not in st.session_state:
    st.session_state["model_name"] = "PAIRWISE:latest"

if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    st.title('ğŸ’¬ EasyJudge')
    st.write('EasyJudge is an innovative tool that uses large language models to precisely assess other models\' responses across multiple dimensions and various scenarios.')
    st.subheader('Models and parameters')
    option = st.selectbox(
        'Select a model',
        [model['name'].replace(':latest', '') for model in model_list['models']], index=1)
    st.write('You selected:', option)
    st.session_state["model_name"] = option
    st.divider()

    temperature = st.slider('temperature', min_value=0.01, max_value=1.0, value=0.1, step=0.01)
    top_p = st.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.slider('max_length', min_value=1024, max_value=8192, value=1024, step=8)
    
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
    st.session_state.clear()  # æ¸…é™¤ st.session_state ä¸­çš„æ‰€æœ‰å†…å®¹
    st.session_state['question_body'] = ""
    st.session_state['answer1_body'] = ""
    st.session_state['answer2_body'] = ""
    st.session_state['reference'] = ""
    
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


st.title(f"Judged by {st.session_state['model_name']} Model")

uploaded_file = st.file_uploader(
        "",
        type=["json", "jsonl"],
        help="Scanned documents are not supported yet!",
    )

options = ['default', 'analyzing_general', 'asking_how_to_question', 'brainstorming', 'chitchat', 'classification_identification', 'code_correction_rewriting', 'code_generation', 'code_to_code_translation', 'counterfactual', 'creative_writing', 'data_analysis', 'explaining_code', 'explaining_general', 'functional_writing', 'information_extraction', 'instructional_rewriting', 'keywords_extraction', 'language_polishing', 'math_reasoning', 'open_question', 'paraphrasing', 'planning', 'question_generation', 'ranking', 'reading_comprehension', 'recommendation', 'roleplay', 'seeking_advice', 'solving_exam_question_with_math', 'solving_exam_question_without_math', 'text_correction', 'text_simplification', 'text_summarization', 'text_to_text_translation', 'title_generation', 'topic_modeling', 'value_judgement', 'verifying_fact', 'writing_advertisement', 'writing_cooking_recipe', 'writing_email', 'writing_job_application', 'writing_news_article', 'writing_personal_essay', 'writing_presentation_script', 'writing_product_description', 'writing_social_media_post', 'writing_song_lyrics']

# åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªé€‰æ‹©çš„å˜é‡
if 'selected_option' not in st.session_state:
    st.session_state.selected_option = options[0]

def update_selection(option):
    st.session_state.selected_option = option

# ä½¿ç”¨expanderæ¥ç»„ç»‡æ˜¾ç¤ºï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶æŠ˜å å’Œå±•å¼€
with st.expander("Choose an scenario"):
    # è®¡ç®—éœ€è¦çš„è¡Œå’Œåˆ—æ•°é‡
    num_rows = len(options) // 8 + (1 if len(options) % 8 > 0 else 0)
    rows = [st.columns(8) for _ in range(num_rows)]
    option_index = 0

    for row in rows:
        for col in row:
            with col:
                # ä»…åœ¨è¿˜æœ‰é€‰é¡¹æ—¶æ˜¾ç¤ºå•é€‰æŒ‰é’®
                if option_index < len(options):
                    # æ£€æŸ¥è¿™ä¸ªé€‰é¡¹æ˜¯å¦è¢«é€‰ä¸­
                    is_checked = st.radio(
                        "", [options[option_index]],
                        key=f"option_{option_index}",  # ç¡®ä¿æ¯ä¸ªå•é€‰æŒ‰é’®ç»„çš„keyä¸åŒ
                        index=0 if st.session_state.selected_option == options[option_index] else None,
                        on_change=update_selection,
                        args=(options[option_index],)
                    )
                    option_index += 1

# æ˜¾ç¤ºé€‰ä¸­çš„é€‰é¡¹
st.write("You selected:", st.session_state.selected_option)

# å®šä¹‰ç»´åº¦è¯„ä¼°å¤é€‰æ¡†çš„é€‰é¡¹
options_group_1 = [
    "text quality",
    "math correctness",
    "depth of understanding",
    "harmlessness",
    "information richness",
    "accuracy",
    "code correctness",
    "instruction following",
    "structure of answer",
    "Logical Reasoning"
]
options_group_2 = [
    "attractive",
    "variety",
    "concise",
    "professional",
    "writing style",
    "language style",
    "persuasive language",
    "vivid",
    "user-friendly",
    "tone"
]
options_group_3 = [
    "Clarity",
    "Relevance to Topic/Text",
    "Depth",
    "Coherence",
    "Originality",
    "Instruction Following",
    "Fluency",
    "Engagement",
    "Detail",
    "Creativity"
]
options_group_4 = [
    "Structure",
    "Readability",
    "Formatting and Layout",
    "Introduction",
    "Brief Summary at the Beginning",
    "Headline",
    "Length",
    "Hashtags",
    "signature",
    "visuals"
]

# ç”¨äºå­˜å‚¨ç”¨æˆ·é€‰æ‹©çš„é€‰é¡¹ï¼Œä½¿ç”¨æ–°çš„å˜é‡å criteria_selected_option
if 'criteria_selected_option' not in st.session_state:
    st.session_state.criteria_selected_option = {
        "group_1": [],
        "group_2": [],
        "group_3": [],
        "group_4": []
    }

# è®¡ç®—æ€»çš„é€‰é¡¹æ•°é‡
total_selected = sum(len(st.session_state.criteria_selected_option[group]) for group in st.session_state.criteria_selected_option)

# è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œè¶…è¿‡ 10 ä¸ªé€‰é¡¹æ—¶ç¦ç”¨å¤é€‰æ¡†
disable_checkboxes = total_selected >= 10

# åˆ›å»ºä¸€ä¸ªåŒ…å« 4 ç»„å¤é€‰æ¡†çš„æ¨ªå‘æ’åˆ—
with st.expander("Select evaluation criteria"):
    # ä½¿ç”¨ st.columns åˆ›å»º 4 åˆ—å¸ƒå±€
    cols = st.columns(4)

    # åœ¨æ¯åˆ—çš„é¡¶éƒ¨æ·»åŠ ç»„åç§°
    with cols[0]:
        st.write("basic standard")
    with cols[1]:
        st.write("style")
    with cols[2]:
        st.write("content")
    with cols[3]:
        st.write("format")

    # åœ¨æ¯ä¸€åˆ—ä¸­æ”¾ç½®å¤é€‰æ¡†
    for i in range(10):
        # ç¬¬ 1 ç»„å¤é€‰æ¡†
        with cols[0]:
            option = options_group_1[i]
            checked = option in st.session_state.criteria_selected_option["group_1"]
            if st.checkbox(option, key=f"group_1_{option}", value=checked, disabled=not checked and disable_checkboxes):
                if option not in st.session_state.criteria_selected_option["group_1"]:
                    st.session_state.criteria_selected_option["group_1"].append(option)
            else:
                if option in st.session_state.criteria_selected_option["group_1"]:
                    st.session_state.criteria_selected_option["group_1"].remove(option)

        # ç¬¬ 2 ç»„å¤é€‰æ¡†
        with cols[1]:
            option = options_group_2[i]
            checked = option in st.session_state.criteria_selected_option["group_2"]
            if st.checkbox(option, key=f"group_2_{option}", value=checked, disabled=not checked and disable_checkboxes):
                if option not in st.session_state.criteria_selected_option["group_2"]:
                    st.session_state.criteria_selected_option["group_2"].append(option)
            else:
                if option in st.session_state.criteria_selected_option["group_2"]:
                    st.session_state.criteria_selected_option["group_2"].remove(option)

        # ç¬¬ 3 ç»„å¤é€‰æ¡†
        with cols[2]:
            option = options_group_3[i]
            checked = option in st.session_state.criteria_selected_option["group_3"]
            if st.checkbox(option, key=f"group_3_{option}", value=checked, disabled=not checked and disable_checkboxes):
                if option not in st.session_state.criteria_selected_option["group_3"]:
                    st.session_state.criteria_selected_option["group_3"].append(option)
            else:
                if option in st.session_state.criteria_selected_option["group_3"]:
                    st.session_state.criteria_selected_option["group_3"].remove(option)

        # ç¬¬ 4 ç»„å¤é€‰æ¡†
        with cols[3]:
            option = options_group_4[i]
            checked = option in st.session_state.criteria_selected_option["group_4"]
            if st.checkbox(option, key=f"group_4_{option}", value=checked, disabled=not checked and disable_checkboxes):
                if option not in st.session_state.criteria_selected_option["group_4"]:
                    st.session_state.criteria_selected_option["group_4"].append(option)
            else:
                if option in st.session_state.criteria_selected_option["group_4"]:
                    st.session_state.criteria_selected_option["group_4"].remove(option)


# è¾“å‡ºé€‰ä¸­çš„åç§°
selected_criteria = []
for group in st.session_state.criteria_selected_option:
    selected_criteria.extend(st.session_state.criteria_selected_option[group])

# æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„æ ‡å‡†
if 0 < len(selected_criteria) < 5:
    st.warning("You must select either 0 or at least 5 criteria.")
    disable_other_operations = True
    st.stop() 
else:
    disable_other_operations = False

# åªæœ‰å½“é€‰ä¸­ 0 ä¸ªæˆ– 5 ä¸ªåŠä»¥ä¸Šé€‰é¡¹æ—¶ï¼Œæ‰å…è®¸æ‰§è¡Œå…¶ä»–æ“ä½œ
if not disable_other_operations:
    # ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œ
    if selected_criteria:
        st.write(f"You selected: {', '.join(selected_criteria)}")  # è¾“å‡ºæ ¼å¼ä¸º "You selected: ..."
    else:
        st.write("You selected: No criteria selected.")

#ç”¨æˆ·é€‰æ‹©PAIRWISE
if st.session_state["model_name"] == "PAIRWISE":

    if 'file_processed' not in st.session_state:
        st.session_state.file_processed = False

    #ç”¨æˆ·é€‰æ‹©PAIRSEï¼Œä¸Šä¼ jsonæ–‡ä»¶
    if uploaded_file and not st.session_state.file_processed:
        try:
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹
            uploaded_file.seek(0)
            file_content = uploaded_file.read().decode('utf-8')  # è¯»å–å¹¶è§£ç ä¸ºUTF-8æ ¼å¼çš„å­—ç¬¦ä¸²
            data = json.loads(file_content)  # è§£æJSONæ•°æ®

            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºåˆ—è¡¨
            if isinstance(data, list):
                # æ£€æŸ¥æ¯ä¸ªå…ƒç´ æ˜¯å¦åŒ…å«å¿…è¦çš„é”®
                missing_keys = []
                for i, item in enumerate(data):
                    required_keys = {'question_body', 'answer1_body', 'answer2_body'}
                    if not all(key in item for key in required_keys):
                        missing_keys.append(i)

                if not missing_keys:
                    st.success("All entries are correctly formatted.")
                    # å¯ä»¥åœ¨è¿™é‡Œå¤„ç†æ•°æ®
                else:
                    st.error(f"Missing required keys in entries: {missing_keys}")
                
                st.session_state.file_processed = True
            else:
                st.error("Uploaded file does not contain a JSON array.")
        except json.JSONDecodeError:
            st.error("The file is not a valid JSON file.")
        except Exception as e:
            st.error(f"An error occurred: {e}")

        # å°è¯•ä»æœ¬åœ°æ–‡ä»¶ç³»ç»ŸåŠ è½½æ¨¡æ¿æ–‡ä»¶
        template_file = "./prompt_template/PAIRWISE_WOR.txt"  # æ›´æ¢ä¸ºä½ çš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„
        try:
            with open(template_file, "r") as file:
                base_prompt = file.read()
        except FileNotFoundError:
            st.error(f"Error: {template_file} file not found.")
            st.stop()

        # å¤„ç†ä¸Šä¼ çš„æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ¨¡å‹è°ƒç”¨çš„prompt
        if isinstance(data, list):
            for item in data:
                if all(key in item for key in ['question_body', 'answer1_body', 'answer2_body', 'reference']):
                    question_body=item['question_body'],
                    answer1_body=item['answer1_body'],
                    answer2_body=item['answer2_body'],
                    reference = item['reference']
                    
                    # åœ¨è¿™é‡Œè°ƒç”¨æ¨¡å‹ç”Ÿæˆç»“æœ
                    # ç¤ºä¾‹ï¼šfull_response = call_your_model(final_prompt)
                    final_prompt = base_prompt.format(
                        scenario = st.session_state.selected_option,
                        criteria = read_criteria(st.session_state.selected_option),
                        question_body=question_body,
                        answer1_body=answer1_body,
                        answer2_body=answer2_body,
                        reference= reference if reference else "N/A"  # Use "N/A" or any suitable placeholder if reference is empty
                    )

                    for_chart = {"reference":reference, "answer1":answer1_body, "answer2":answer2_body}

                    # Call ollama.chat to process the dialogue
                    #with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    results_holder = st.empty()

                    full_response = ""

                    with st.spinner('Runing...Please Wait...'):
                        start_output = False
                        # è·å–æ¨¡å‹å“åº”çš„å®Œæ•´å†…å®¹
                        for chunk in ollama.chat(
                            model=st.session_state["model_name"],
                            messages=[{"role": "user", "content": final_prompt}],
                            stream=True
                        ):
                            if 'message' in chunk and 'content' in chunk['message']:
                                full_response += (chunk['message']['content'] or "")
                                message_placeholder.markdown(full_response, unsafe_allow_html=True)

                                # ä¸€æ—¦æ”¶åˆ°ç¬¬ä¸€ä¸ªå†…å®¹å—ï¼Œç»“æŸåŠ è½½æç¤ºï¼ˆé€šè¿‡ç»“æŸ `with` è‡ªåŠ¨æ¶ˆå¤±ï¼‰
                                if not start_output:
                                    start_output = True
                    message_placeholder.empty()

                    # ç»è¿‡å¤„ç†åå†è¾“å‡º
                    processed_response = extract_gpt_response_info_pairwise(full_response)
                    

                    final_result = str(processed_response["final_results"]).replace("Final Result: ", "")
                    result_text = "ğŸ¤ It's a Tie!" if final_result == "Tie" else f"ğŸ† {final_result} Wins!"

                    # ä½¿ç”¨Streamlitçš„columnsè¿›è¡Œå¸ƒå±€ä¼˜åŒ–
                    col1, col2 = st.columns([1, 3])

                    # æ›´æ–°åçš„æ ·å¼
                    common_style = "padding: 20px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
                    background_color1 = "background-color: #f8f9fa;"  # æµ…ç°è‰²
                    background_color2 = "background-color: #e9ecef;"  # è¾ƒæ·±ç°è‰²

                    with col1:
                        st.markdown(f"""
                            <div style="{background_color1} {common_style}">
                                <h2 style="color: #007BFF;">{result_text}</h2>
                            </div>
                            """, unsafe_allow_html=True)

                    with col2:
                        st.markdown(f"""
                            <div style="{background_color2} {common_style}">
                                <h3 style="color: #6c757d;">Detailed Evaluation Feedback</h3>
                                <p style="font-size: 16px; line-height: 1.6;">
                                    {processed_response["Detailed_Evaluation_Feedback"]}
                                </p>
                            </div>
                            """, unsafe_allow_html=True)

                    # # æ·»åŠ åˆ†éš”ç¬¦
                    # st.markdown("<hr>", unsafe_allow_html=True)
                    # æ·»åŠ åˆ†éš”ç¬¦ï¼Œè®¾ç½®æ ·å¼å‡å°‘ä¸Šä¸‹é—´è·
                    st.markdown("<hr style='margin: 10px 0;'>", unsafe_allow_html=True)  # è‡ªå®šä¹‰ä¸Šä¸‹è¾¹è·


                    # æ˜¾ç¤ºç»“æœ
                    #st.write(full_response)

                    # å°†ç»“æœæ·»åŠ åˆ°åŸæ•°æ®ä¸­
                    item['model_critic'] = full_response
                    st.write("successfully written to the JSON file")

            # ä¿å­˜ä¿®æ”¹åçš„æ•°æ®å¹¶æä¾›ä¸‹è½½é“¾æ¥
            modified_file_path = 'critic_by_pairwise_data.json'
            with open(modified_file_path, 'w') as json_file:
                json.dump(data, json_file, indent=4)

            # æä¾›æ–‡ä»¶ä¸‹è½½
            with open(modified_file_path, 'rb') as f:
                #st.download_button('Download critic_by_pairwise_data JSON', f, file_name=modified_file_path)
            
                if st.download_button('Download the evaluation JSON file', f, file_name=modified_file_path):
                    st.success("File downloaded successfully!")
                    st.session_state.file_processed = False
                    #st.experimental_rerun() 

        else:
            st.error("Uploaded data is not a JSON array.")

    else:
        # åˆ›å»ºè¾“å…¥æ¡†
        # å®šä¹‰æˆ–è·å– session_state ä¸­çš„å˜é‡ï¼Œç”¨äºä¿å­˜è¾“å…¥æ¡†å†…å®¹
        if 'question_body' not in st.session_state:
            st.session_state['question_body'] = ""
        if 'answer1_body' not in st.session_state:
            st.session_state['answer1_body'] = ""
        if 'answer2_body' not in st.session_state:
            st.session_state['answer2_body'] = ""
        if 'reference' not in st.session_state:
            st.session_state['reference'] = ""

        # åˆ›å»ºè¾“å…¥æ¡†ï¼Œä½¿ç”¨ session_state å˜é‡ä½œä¸ºå€¼
        question_body = st.text_input("Question:", value=st.session_state['question_body'], key='question_body')
        answer1_body = st.text_input("Answer 1:", value=st.session_state['answer1_body'], key='answer1_body')
        answer2_body = st.text_input("Answer 2:", value=st.session_state['answer2_body'], key='answer2_body')
        reference = st.text_input("Reference:", value=st.session_state['reference'], key='reference')

        # æäº¤æŒ‰é’®ï¼Œç”¨äºå¡«å……æ¨¡æ¿å¹¶æ˜¾ç¤ºç»“æœ
        if st.button("Submit"):
            # æ ¹æ®referenceè¾“å…¥æ˜¯å¦ä¸ºç©ºé€‰æ‹©ä¸åŒçš„æ¨¡æ¿æ–‡ä»¶
            if reference:
                template_file = "./prompt_template/PAIRWISE_WR.txt"
            else:
                template_file = "./prompt_template/PAIRWISE_WOR.txt"
            
            # å°è¯•ä»æ–‡ä»¶åŠ è½½åŸºæœ¬promptæ¨¡æ¿
            try:
                with open(template_file, "r") as file:
                    base_prompt = file.read()
            except FileNotFoundError:
                st.error(f"Error: {template_file} file not found.")
                st.stop()
            
            # ä½¿ç”¨è¯»å–çš„æ¨¡æ¿å’Œç”¨æˆ·è¾“å…¥ç”Ÿæˆæœ€ç»ˆçš„prompt
            # Format the prompt based on input and template
            if not selected_criteria :
                final_prompt = base_prompt.format(
                    scenario = st.session_state.selected_option,
                    criteria = read_criteria(st.session_state.selected_option),
                    question_body=question_body,
                    answer1_body=answer1_body,
                    answer2_body=answer2_body,
                    reference=reference if reference else "N/A"  # Use "N/A" or any suitable placeholder if reference is empty
                )
            else:
                final_prompt = base_prompt.format(
                    scenario = st.session_state.selected_option,
                    criteria = user_selected_criteria(selected_criteria),
                    question_body=question_body,
                    answer1_body=answer1_body,
                    answer2_body=answer2_body,
                    reference=reference if reference else "N/A"  # Use "N/A" or any suitable placeholder if reference is empty
                )

            for_chart = {"reference":reference, "answer1":answer1_body, "answer2":answer2_body}

            # Call ollama.chat to process the dialogue
            #with st.chat_message("assistant"):
            message_placeholder = st.empty()
            results_holder = st.empty()

            full_response = ""

            with st.spinner('Runing...Please Wait...'):
                start_output = False
                # è·å–æ¨¡å‹å“åº”çš„å®Œæ•´å†…å®¹
                for chunk in ollama.chat(
                    model=st.session_state["model_name"],
                    messages=[{"role": "user", "content": final_prompt}],
                    stream=True
                ):
                    if 'message' in chunk and 'content' in chunk['message']:
                        full_response += (chunk['message']['content'] or "")
                        message_placeholder.markdown(full_response, unsafe_allow_html=True)

                        # ä¸€æ—¦æ”¶åˆ°ç¬¬ä¸€ä¸ªå†…å®¹å—ï¼Œç»“æŸåŠ è½½æç¤ºï¼ˆé€šè¿‡ç»“æŸ `with` è‡ªåŠ¨æ¶ˆå¤±ï¼‰
                        if not start_output:
                            start_output = True
            message_placeholder.empty()

            # ç»è¿‡å¤„ç†åå†è¾“å‡º
            processed_response = extract_gpt_response_info_pairwise(full_response)

            # ç»“æœæ˜¾ç¤º
            final_result = str(processed_response["final_results"]).replace("Final Result: ", "")
            result_text = "ğŸ¤ It's a Tie!" if final_result == "Tie" else f"ğŸ† {final_result} Wins!"

            # ä½¿ç”¨Streamlitçš„columnsè¿›è¡Œå¸ƒå±€ä¼˜åŒ–
            col1, col2 = st.columns([1, 3])

            # æ›´æ–°åçš„æ ·å¼
            common_style = "padding: 20px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
            background_color1 = "background-color: #f8f9fa;"  # æµ…ç°è‰²
            background_color2 = "background-color: #e9ecef;"  # è¾ƒæ·±ç°è‰²

            with col1:
                st.markdown(f"""
                    <div style="{background_color1} {common_style}">
                        <h2 style="color: #007BFF;">{result_text}</h2>
                    </div>
                    """, unsafe_allow_html=True)

            with col2:
                st.markdown(f"""
                    <div style="{background_color2} {common_style}">
                        <h3 style="color: #6c757d;">Detailed Evaluation Feedback</h3>
                        <p style="font-size: 16px; line-height: 1.6;">
                            {processed_response["Detailed_Evaluation_Feedback"]}
                        </p>
                    </div>
                    """, unsafe_allow_html=True)

            # # æ·»åŠ åˆ†éš”ç¬¦
            # st.markdown("<hr>", unsafe_allow_html=True)
            # æ·»åŠ åˆ†éš”ç¬¦ï¼Œè®¾ç½®æ ·å¼å‡å°‘ä¸Šä¸‹é—´è·
            st.markdown("<hr style='margin: 10px 0;'>", unsafe_allow_html=True)  # è‡ªå®šä¹‰ä¸Šä¸‹è¾¹è·

            # ç”»åˆ†æ•°çš„å¯¹æ¯”æ¡å½¢å›¾
            st.markdown("### Score Comparison Breakdown ğŸ“Š")
            fig1, fig2, fig3 = plot_scores_PAIRWISE(processed_response, for_chart)

            # åˆ©ç”¨ä¸‰åˆ—æ¥æ˜¾ç¤ºå›¾è¡¨
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("<h5 style='margin: 0;'>Score Comparison for A vs B</h5>", unsafe_allow_html=True)  # ä½¿ç”¨h5æ ‡ç­¾æ¥å‡å°å­—ä½“
                st.plotly_chart(fig1, use_container_width=True)

            with col2:
                st.markdown("<h5 style='margin: 0;'>Comparison of Answer 1 with Reference</h5>", unsafe_allow_html=True)  # ä½¿ç”¨h5æ ‡ç­¾æ¥å‡å°å­—ä½“
                st.plotly_chart(fig2, use_container_width=True)

            with col3:
                st.markdown("<h5 style='margin: 0;'>Comparison of Answer 2 with Reference</h5>", unsafe_allow_html=True)  # ä½¿ç”¨h5æ ‡ç­¾æ¥å‡å°å­—ä½“
                st.plotly_chart(fig3, use_container_width=True)

else:

    # åˆ›å»ºè¾“å…¥æ¡†
    question_body = st.text_input("Question:")
    answer_body = st.text_input("Answer:")
    reference = st.text_input("Reference:")

    # æäº¤æŒ‰é’®ï¼Œç”¨äºå¡«å……æ¨¡æ¿å¹¶æ˜¾ç¤ºç»“æœ
    if st.button("Submit"):
        # æ ¹æ®referenceè¾“å…¥æ˜¯å¦ä¸ºç©ºé€‰æ‹©ä¸åŒçš„æ¨¡æ¿æ–‡ä»¶
        if reference:
            template_file = "./prompt_template/POINTWISE_WR.txt"
        else:
            template_file = "./prompt_template/POINTWISE_WOR.txt"
        
        # å°è¯•ä»æ–‡ä»¶åŠ è½½åŸºæœ¬promptæ¨¡æ¿
        try:
            with open(template_file, "r") as file:
                base_prompt = file.read()
        except FileNotFoundError:
            st.error(f"Error: {template_file} file not found.")
            st.stop()
        
        # ä½¿ç”¨è¯»å–çš„æ¨¡æ¿å’Œç”¨æˆ·è¾“å…¥ç”Ÿæˆæœ€ç»ˆçš„prompt
        # Format the prompt based on input and template
        if not selected_criteria:
            final_prompt = base_prompt.format(
                scenario = st.session_state.selected_option,
                criteria = read_criteria(st.session_state.selected_option),
                question_body=question_body,
                answer_body=answer_body,
                reference=reference if reference else "N/A"  # Use "N/A" or any suitable placeholder if reference is empty
            )
        else:
            final_prompt = base_prompt.format(
                scenario = st.session_state.selected_option,
                criteria = user_selected_criteria(selected_criteria),
                question_body=question_body,
                answer_body=answer_body,
                reference=reference if reference else "N/A"  # Use "N/A" or any suitable placeholder if reference is empty
            )

        for_chart = {"reference":reference, "answer":answer_body}

        score_placeholder = st.empty()
        message_placeholder = st.empty() 
        full_response = ""

        # æ˜¾ç¤ºåŠ è½½æç¤ºï¼ˆæ­£åœ¨æ‰§è¡Œï¼‰
        with st.spinner('Runing...Please Wait...'):
            start_output = False

            # è·å–æ¨¡å‹å“åº”çš„å®Œæ•´å†…å®¹
            for chunk in ollama.chat(
                model=st.session_state["model_name"],
                messages=[{"role": "user", "content": final_prompt}],
                stream=True
            ):
                if 'message' in chunk and 'content' in chunk['message']:
                    full_response += (chunk['message']['content'] or "")
                    message_placeholder.markdown(full_response, unsafe_allow_html=True)

                    # ä¸€æ—¦æ”¶åˆ°ç¬¬ä¸€ä¸ªå†…å®¹å—ï¼Œç»“æŸåŠ è½½æç¤ºï¼ˆé€šè¿‡ç»“æŸ `with` è‡ªåŠ¨æ¶ˆå¤±ï¼‰
                    if not start_output:
                        start_output = True
        message_placeholder.empty()
        # å¤„ç†å“åº”
        processed_response = extract_gpt_response_info_pointwise(full_response)

        # ç»“æœæ˜¾ç¤º
        print(processed_response)
        overall_score = processed_response["Overall_Score"]["Overall Score"]
        result_text = f'ğŸ“ Final Score: <span style="color: #FF4500;">{overall_score}/10</span></h2>'

        # ä½¿ç”¨Streamlitçš„columnsè¿›è¡Œå¸ƒå±€ä¼˜åŒ–
        col1, col2 = st.columns([1, 3])

        # æ›´æ–°åçš„æ ·å¼
        common_style = "padding: 20px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
        background_color1 = "background-color: #f8f9fa;"  # æµ…ç°è‰²
        background_color2 = "background-color: #e9ecef;"  # è¾ƒæ·±ç°è‰²

        with col1:
            st.markdown(f"""
                <div style="{background_color1} {common_style}">
                    <h2 style="color: #007BFF;">{result_text}</h2>
                </div>
                """, unsafe_allow_html=True)

        with col2:
            st.markdown(f"""
                <div style="{background_color2} {common_style}">
                    <h3 style="color: #6c757d;">Detailed Evaluation Feedback</h3>
                    <p style="font-size: 16px; line-height: 1.6;">
                        {processed_response["Detailed_Evaluation_Feedback"]}
                    </p>
                </div>
                """, unsafe_allow_html=True)

        # æ·»åŠ åˆ†éš”ç¬¦ï¼Œè®¾ç½®æ ·å¼å‡å°‘ä¸Šä¸‹é—´è·
        st.markdown("<hr style='margin: 10px 0;'>", unsafe_allow_html=True)  # è‡ªå®šä¹‰ä¸Šä¸‹è¾¹è·

        # ç”»åˆ†æ•°çš„å¯¹æ¯”æ¡å½¢å›¾
        st.markdown("### Score Comparison Breakdown ğŸ“Š")
        fig1, fig2= plot_scores_POINTWISE(processed_response, for_chart)

        # åˆ©ç”¨ä¸‰åˆ—æ¥æ˜¾ç¤ºå›¾è¡¨
        col1, col2= st.columns(2)
        with col1:
            st.markdown("<h5 style='margin: 0;'>Score Comparison for A vs B</h5>", unsafe_allow_html=True)  # ä½¿ç”¨h5æ ‡ç­¾æ¥å‡å°å­—ä½“
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            st.markdown("<h5 style='margin: 0;'>Comparison of Answer with Reference</h5>", unsafe_allow_html=True)  # ä½¿ç”¨h5æ ‡ç­¾æ¥å‡å°å­—ä½“
            st.plotly_chart(fig2, use_container_width=True)